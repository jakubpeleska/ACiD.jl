<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · ACiD.jl</title><meta name="title" content="Home · ACiD.jl"/><meta property="og:title" content="Home · ACiD.jl"/><meta property="twitter:title" content="Home · ACiD.jl"/><meta name="description" content="Documentation for ACiD.jl."/><meta property="og:description" content="Documentation for ACiD.jl."/><meta property="twitter:description" content="Documentation for ACiD.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">ACiD.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href="index.html">Home</a><ul class="internal"><li><a class="tocitem" href="#Usage"><span>Usage</span></a></li><li><a class="tocitem" href="#Functions-Index"><span>Functions Index</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="index.html">Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="index.html">Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/jakubpeleska/ACiD.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/jakubpeleska/ACiD.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ACiD.jl"><a class="docs-heading-anchor" href="#ACiD.jl">ACiD.jl</a><a id="ACiD.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ACiD.jl" title="Permalink"></a></h1><p>This is an implementation of an asynchronous multiprocessing optimization algorithm with a continuous local momentum called <strong>A²CiD²</strong> in the Julia programming language as introduced in [[1]].</p><blockquote><p>[!NOTE] There is also an official demo by the original authors of [[1]] in Python: <a href="https://github.com/AdelNabli/ACiD &quot;AdelNabli/ACiD: Implementation of NeurIPS 2023 paper ACiD: Accelerating Asynchronous Communication in Decentralized Deep Learning.&quot;">AdelNabli/ACiD</a></p></blockquote><p>&lt;!– </p><h2 id="Usage"><a class="docs-heading-anchor" href="#Usage">Usage</a><a id="Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Usage" title="Permalink"></a></h2><h3 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h3><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">s = &quot;Julia syntax highlighting&quot;;
println(s);</code></pre><p>–&gt;</p><h2 id="Functions-Index"><a class="docs-heading-anchor" href="#Functions-Index">Functions Index</a><a id="Functions-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-Index" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.acid_ode" href="#ACiD.acid_ode"><code>ACiD.acid_ode</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>Integrate the ODE for the continuous momentum, see https://arxiv.org/pdf/2306.08289.pdf for details.</strong></p><p>Update parameters (params<em>com and params</em>com_tilde) in-place.</p><p>Parameters:</p><ul><li>params_com (torch.tensor): 1D tensor containing all of the models learnable parameters.</li><li>params<em>com</em>tilde (torch.tensor): &quot;momentum&quot; variable, same size as params<em>com, mixing with params</em>com to obtain acceleration.</li><li>ode<em>matrix (torch.tensor): a 2x2 matrix storing the parameters of the linear mixing between params and params</em>tilde.</li><li>t_old (float): time of the last local update.</li><li>t_new (float): time of the current update.</li><li>delta<em>t</em>grad (float): time that it takes to compute a grad step. Used to re-normalize time, as done in the paper.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_averaging.jl#L12-L23">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.gossip_process" href="#ACiD.gossip_process"><code>ACiD.gossip_process</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>Gossip routine for the p2p averaging of the model&#39;s parameters running in the background.</strong></p><ul><li>Average the parameters of all the workers at the beginning (to start from a common initialization), and at the end.</li><li>Use the mp.Variable &quot;rank_other&quot; to communicate with the orchestring process   that pairs available workers together to perform p2p communications, allowing this   function to know with which rank to communicate next.</li><li>Depending on deterministic_com, implement or not a P.P.P for the communication process:   if true, a random number of p2p communications between 2 grad steps are done, following a poisson law.</li><li>When the orchestrating process counted that the right number of grad step have been performed in total,   signal it back to this process (stops the communication routine), which signals to the main process to stop performing grad steps.</li></ul><p>Parameters:</p><ul><li>rank (int): our rank id in the distributed setting.</li><li>local_rank (int): the local rank of the worker inside its compute node (to create a Cuda Stream in the right GPU).</li><li>world_size (int): the total number of workers.</li><li>rank<em>other (mp.Value): a multiprocessing Value to store the id of the rank of the next communication. It is updated                           by the orchestrating process pairing workers together, and re-initialized by this one after a communication.                           if rank</em>other.value == -1: (base value) no peer has been found yet.                           if rank<em>other.value == -2: signal from the orchestrating process that enough gradients have been computed in total,                                                   stops the communication process.                           if rank</em>other.value not in [-1, -2]: contains the rank of the worker we are supposed to communicate with next.</li><li>params_com (torch.tensor): 1D tensor containing the model&#39;s parameters.</li><li>params<em>other (torch.tensor): 1D tensor, placeholder to receive the params</em>com of the worker with whom we communicate.</li><li>barrier<em>sync</em>averaging (mp.Barrier): a barrier used to communicate with the synchronization process.                                       When we meet this barrier, we signal to the sync process that we finished our previous communication,                                       and are available for the next one, so that it can begin to look for another available peer to connect                                       to for the next p2p communication.</li><li>continue<em>grad</em>routine (mp.Value containing a bool): whether or not the grad process should continue.                                                       Initialized at 1 (true). Is put to 0 (False) when the orchestrating                                                       process signals to us that the total number of gradients quota has been met.</li><li>barrier<em>end</em>init (mp.Barrier): a barrier to signal to the <strong>init</strong> function of ADP&#39;s class that the initializing average of the parameters                                   has been performed, and that ADP can resume its init.</li><li>barrier<em>com</em>grad (mp.Barrier): a barrier to make sure a certain amount of communication has been made between 2 grads.                                   Also used to make sure a certain amount of grad have been performed between 2 comm if rate_com &lt; 1.</li><li>log (logger): to print messages in the logs if needed.</li><li>com<em>history (list of mp.Value): list of size world</em>size. Used to logg how many times this worker communicated with each of its peers.</li><li>count<em>coms</em>local (mp.Value): a count of the number of p2p communications this worker has done.</li><li>rate_com (float): the rate at which p2p communications are done (in expectation) compared to local grad steps.</li><li>apply_acid (bool): whether or not to apply ACiD momentum. If True, the communication is an &quot;event&quot; triggering a momentum update.</li><li>params<em>com</em>tilde (torch.tensor): &quot;momentum&quot; variable, same size as params<em>com, mixing with params</em>com to obtain acceleration.</li><li>ode<em>matrix (torch.tensor): a 2x2 matrix storing the parameters of the linear mixing between params and params</em>tilde.</li><li>t<em>last</em>spike (float): time of the last local update to params_com (be it a communication or gradient one).</li><li>delta<em>t</em>grad (mp.Value storing a double): the variable keeping track of the time that it takes to make a grad step.</li><li>beta<em>tilde (float): the lpha</em>tilde value to use in ACiD.</li><li>deterministic_com (bool): whether or not to schedule to use Poisson Point Processes for the communications.                           if True, a random number of p2p communications between 2 grad steps are done, following a poisson law.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_averaging.jl#L158-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.master_process" href="#ACiD.master_process"><code>ACiD.master_process</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>Orchestrating process hosted on worker 0.</strong></p><p>This process accomplishes 2 things:</p><ul><li>Group available workers by pairs for p2p communication, according to the given graph topology, and trying to minimize latency   by pairing together workers that were the first to be available to communicate.</li><li>Signal to all processes when the target number of grads have been reached, so that computations &amp; communication can stop.</li></ul><p>Parameters:</p><ul><li>world_size (int): the total number of workers.</li><li>nb<em>grad</em>tot_goal (int): The target number of total nb of grads performed by all workers.                           When it is reached, this process sends the signal to all other to stop all computations &amp; communications.</li><li>log (logger): to print messages in the logs if needed.</li><li>graph_topology (str): Graph topology to use to make p2p communication (dictates which edges can be used).                       Currently supports either of [&#39;complete&#39;].</li><li>deterministic_neighbor (bool): whether or not to schedule the p2p communications.                                   if True, if at the next step, worker i is supposed to communicate with j,                                   i will wait for j to be available to communicate.                                   if False, i will communicate faster, by just picking one of its available neighbor.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_sync.jl#L147-L165">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.sync_process" href="#ACiD.sync_process"><code>ACiD.sync_process</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>Process run by every worker in the background.</strong></p><p>This process allows each worker to communicate with the &quot;orchestrating&quot; master process (hosted by worker 0). The goal is to signal to the master process when this worker is available for communication, and to gather from the master process the rank of the peer with which we are supposed to communicate. When received, this information &quot;rank<em>other&quot; is sent to the p2p</em>averaging process run in parallel in this worker, so that the p2p averaging process knows with which worker to communicate next. This process will communicate with one of the world<em>size &quot;listen</em>given<em>rank&quot; processes hosted at worker 0, which has world</em>size + 1 processes run in parallel:     * one to &quot;listen<em>to&quot; each one of the sync</em>process run by each worker.     * one &quot;orchestrating&quot; process, dedicated to make pairs of workers. So, in total, there are 2*world<em>size + 1 processes that need to communicate with each other (only sending ints), so initialize a process</em>group using gloo backend here.</p><p>Parameters:</p><ul><li>rank (int): our rank id in the distributed setting.</li><li>world_size (int): the total number of workers.</li><li>rank<em>other (mp.Value): a multiprocessing Value to store the id of the rank of the next communication.                           It is updated here, based on the information given by the master process, to signal to the p2p</em>averaging process                           run in parallel in this worker which peer to communicate wiith next.                           if rank_other.value == -2: signal from the orchestrating process that enough gradients have been computed in total,                                                   stops the communication process.</li><li>new_grads (mp.Value): a multiprocessing Value updated by the process and the main one, counting how many new grad steps have been performed                       by this worker since last communication. This is used by the master process to count the total number of grad done,                       and initiate the &quot;kill&quot; of all processes when the right amount of grad steps have been performed in total.</li><li>barrier<em>sync</em>averaging (mp.Barrier): a barrier used to communicate with the p2p_averaging process.                                       When the averaging process meets this barrier, it signals to this process that the worker                                       is available for the next communication, so we can begin to look for another available peer to connect                                       to by sending our rank information to the master process which will realize the pairing.</li><li>log (logger): to print messages in the logs if needed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_sync.jl#L14-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.do_send" href="#ACiD.do_send"><code>ACiD.do_send</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>The send THEN receive function.</strong></p><p>Expects that the peer with whom we communicate runs the symetric function receive THEN send. The p2p communication edits in-place the values of the parameters params<em>com and params</em>com<em>tilde (if apply</em>acid).</p><p>Parameters:</p><ul><li>params_com (torch.tensor): 1D tensor containing the model&#39;s parameters.</li><li>params<em>other</em>worker (torch.tensor): 1D tensor, placeholder to receive the params_com of the worker with whom we communicate.</li><li>process<em>group (a torch distributed process</em>group): specifies the process_group to use for the p2p communications.</li><li>other_rank (int): the rank of the worker we communicate with.</li><li>apply_acid (bool): whether or not to apply ACiD momentum. If true, the communication is an &quot;event&quot; triggering a momentum update.</li><li>params<em>com</em>tilde (torch.tensor): &quot;momentum&quot; variable, same size as params<em>com, mixing with params</em>com to obtain acceleration.</li><li>ode<em>matrix (torch.tensor): a 2x2 matrix storing the parameters of the linear mixing between params and params</em>tilde.</li><li>t<em>last</em>spike (float): time of the last local update to params_com (be it a communication or gradient one).</li><li>delta<em>t</em>grad (mp.Value storing a double): the variable keeping track of the time that it takes to make a grad step.</li><li>beta<em>tilde (float): the lpha</em>tilde value to use in ACiD.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_averaging.jl#L43-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.do_recv" href="#ACiD.do_recv"><code>ACiD.do_recv</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>The receive THEN send function.</strong></p><p>Expects that the peer with whom we communicate runs the symetric function send THEN receive. The p2p communication edits in-place the values of the parameters params<em>com and params</em>com<em>tilde (if apply</em>acid).</p><p>Parameters:</p><ul><li>params_com (torch.tensor): 1D tensor containing the model&#39;s parameters.</li><li>params<em>other</em>worker (torch.tensor): 1D tensor, placeholder to receive the params_com of the worker with whom we communicate.</li><li>process<em>group (a torch distributed process</em>group): specifies the process_group to use for the p2p communications.</li><li>other_rank (int): the rank of the worker we communicate with.</li><li>apply_acid (bool): whether or not to apply ACiD momentum. If true, the communication is an &quot;event&quot; triggering a momentum update.</li><li>params<em>com</em>tilde (torch.tensor): &quot;momentum&quot; variable, same size as params<em>com, mixing with params</em>com to obtain acceleration.</li><li>ode<em>matrix (torch.tensor): a 2x2 matrix storing the parameters of the linear mixing between params and params</em>tilde.</li><li>t<em>last</em>spike (float): time of the last local update to params_com (be it a communication or gradient one).</li><li>delta<em>t</em>grad (mp.Value storing a double): the variable keeping track of the time that it takes to make a grad step.</li><li>beta<em>tilde (float): the lpha</em>tilde value to use in ACiD.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_averaging.jl#L100-L116">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ACiD.listen_given_rank" href="#ACiD.listen_given_rank"><code>ACiD.listen_given_rank</code></a> — <span class="docstring-category">Function</span></header><section><div><p><strong>Process run in the background of worker 0.</strong></p><p>Its goal is to listen to one specific worker (specifically, its &quot;sync<em>process&quot; process), and to send it information coming from the orchestrating process also hosted by worker 0. The main goal of this function is to put to the mp.Queue the rank of the worker it is listening to when this worker sent, through its &quot;sync</em>process&quot; function, the signal that its corresponding worker was available for a communication. Then, as this mp.Queue is shared with the orchestrating process, the orchestrating process can receive the information and pair the worker with another one.</p><p>Parameters:</p><ul><li>rank (int): our rank id in the distributed setting.</li><li>world_size (int): the total number of workers.</li><li>queue (mp.Queue): queue containing the ranks of all available workers for communication.                   The orchestrating process then only needs to &quot;de-queue&quot; the ranks to make pairs, insuring that the communications are performed in FIFO style,                   minimizing latency.</li><li>nb<em>grad</em>tot<em>so</em>far (mp.Value): int storing the global count of grads (total number of gradients taken by all workers).                                   This value is updated by adding to it the &quot;new<em>grads&quot; (see &quot;sync</em>process&quot; doc) from every worker.                                   This mp.Value is thus updated by world<em>size &quot;listen</em>given_rank&quot; processes, and used by the orchestrating process to kill all processes                                   when the target number of grads is reached.</li><li>lock (mp.Lock): multiprocessing lock to make sure that the nb<em>grad</em>tot<em>so</em>far is edited by only one process at a time, so that no &quot;new gradients&quot; are thrown out                   by a multiprocessing bug.</li><li>log (logger): to print messages in the logs if needed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jakubpeleska/ACiD.jl/blob/a2c3b22245ecbbf1f4f4aa884568bee3e13f8f1a/src/p2p_sync.jl#L88-L108">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>[1] &lt;span id=&quot;[1]&quot;&gt;A. Nabli, E. Belilovsky, and E. Oyallon, “<strong>A²CiD²</strong>: Accelerating Asynchronous Communication in Decentralized Deep Learning,” in <em>Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023. [Online]. Available: <a href="https://arxiv.org/abs/2306.08289"><img src="https://img.shields.io/badge/arXiv-2306.08289-b31b1b.svg" alt="arXiv"/></a> <a href="https://doi.org/10.48550/arXiv.2306.08289"><img src="https://img.shields.io/badge/DOI-10.48550/arXiv.2306.08289-b31b1b.svg" alt="DOI:10.48550/ARXIV.2306.08289"/></a>&lt;/span&gt;</p><p>[1]: #[1] &quot;A. Nabli, E. Belilovsky, and E. Oyallon, “A²CiD²: Accelerating Asynchronous Communication in Decentralized Deep Learning,” in Thirty-seventh Conference on Neural Information Processing Systems, 2023.&quot;</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Wednesday 14 February 2024 04:40">Wednesday 14 February 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
